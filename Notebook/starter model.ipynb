{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starter Project\n",
    "## Creating a Basic LSTM Classifier using Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.1 Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from wordsegment import segment, load\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.1.1 Initializing Tokenizer, in case you don't know what a tokenizer is please visit: https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in our model we will be using tweet tokenizer\n",
    "# we have reduced the lenght of the text, and using preserve_case as False we turn all charecters to lower case\n",
    "tknzr = TweetTokenizer(reduce_len=True, preserve_case=False, strip_handles=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.1.2 Creating the Pre-Processing Function here we remove unwanted symbols and numbers from our text and change known emoji symbols to their text value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess(text):\n",
    "    text = str(text)\n",
    "    FLAGS = re.MULTILINE | re.DOTALL\n",
    "    eyes = r\"[8:=;]\"\n",
    "    nose = r\"['`\\-]?\"\n",
    "\n",
    "    def re_sub(pattern, repl):\n",
    "        return re.sub(pattern, repl, text, flags=FLAGS)\n",
    "    text = re_sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \"<url>\")\n",
    "    text = re_sub(r\"/\",\" / \")\n",
    "    text = re_sub(r\"@\\w+\", \"<user>\")\n",
    "    text = re_sub(r\"{}{}[)dD]+|[)dD]+{}{}\".format(eyes, nose, nose, eyes), \"<smile>\")\n",
    "    text = re_sub(r\"{}{}p+\".format(eyes, nose), \"<lolface>\")\n",
    "    text = re_sub(r\"{}{}\\(+|\\)+{}{}\".format(eyes, nose, nose, eyes), \"<sadface>\")\n",
    "    text = re_sub(r\"{}{}[\\/|l*]\".format(eyes, nose), \"<neutralface>\")\n",
    "    text = re_sub(r\"<3\",\"<heart>\")\n",
    "    text = re_sub(r\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \"<number>\")\n",
    "    text = re_sub(r\"([!?.]){2,}\", r\"\\1 <repeat>\")\n",
    "    text = re_sub(r\"\\b(\\S*?)(.)\\2{2,}\\b\", r\"\\1\\2 <elong>\")\n",
    "    text = \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "\n",
    "    tokens = tknzr.tokenize(text.lower())\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.1.3 Importing the dataset and pre-processing it\n",
    "#### We turn the sentiment labels to numerical values for the machine to understand\n",
    "#### You can download the dataset from [here](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we import the dataset using read_csv function from the pandas library\n",
    "# just replace INPUTPATH with your input path\n",
    "df = pd.read_csv(\"INPUTPATH.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing the firt 5 rows of our dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding sentiment labels to numerical values\n",
    "encode_label = {'negative' : 0, 'positive' : 1}\n",
    "df['sentiment'] = df['sentiment'].map(encode_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing the firt 5 rows of our dataset again\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing the dataset\n",
    "df['review'] = df['review'].apply(text_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing the firt 5 rows of our dataset again after pre-processing\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.1.4 Saving the pre-processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the to_csv function from the pandas library to export our dataframe as a csv file\n",
    "#just replace destination_folder with your outputpath\n",
    "# Trim text and titletext to first_n_words\n",
    "train_test_ratio = 0.10\n",
    "train_valid_ratio = 0.80\n",
    "\n",
    "first_n_words = 500\n",
    "df_raw['review'] = df_raw['review'].apply(trim_string)\n",
    "\n",
    "# Splits dataset according to label\n",
    "df_real = df_raw[df_raw['sentiment'] == 0]\n",
    "df_fake = df_raw[df_raw['sentiment'] == 1]\n",
    "\n",
    "# Train-test split\n",
    "df_real_full_train, df_real_test = train_test_split(df_real, train_size = train_test_ratio, random_state = 1)\n",
    "df_fake_full_train, df_fake_test = train_test_split(df_fake, train_size = train_test_ratio, random_state = 1)\n",
    "\n",
    "# Train-valid split\n",
    "df_real_train, df_real_valid = train_test_split(df_real_full_train, train_size = train_valid_ratio, random_state = 1)\n",
    "df_fake_train, df_fake_valid = train_test_split(df_fake_full_train, train_size = train_valid_ratio, random_state = 1)\n",
    "\n",
    "# Concatenate splits of different labels\n",
    "df_train = pd.concat([df_real_train, df_fake_train], ignore_index=True, sort=False)\n",
    "df_valid = pd.concat([df_real_valid, df_fake_valid], ignore_index=True, sort=False)\n",
    "df_test = pd.concat([df_real_test, df_fake_test], ignore_index=True, sort=False)\n",
    "\n",
    "# Write preprocessed data for train, test and validation\n",
    "df_train.to_csv(destination_folder + '/train.csv', index=False)\n",
    "df_valid.to_csv(destination_folder + '/valid.csv', index=False)\n",
    "df_test.to_csv(destination_folder + '/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.1 Training the Starter Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing important libraries needed for this section\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Preliminaries\n",
    "\n",
    "from torchtext.data import Field, TabularDataset, BucketIterator\n",
    "\n",
    "# Models\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "# Training\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "#Sets device as GPU or CPU for training depending on your local machine\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.1.1 Loading the pre-processed dataset and creating Training, Testing and Validation Sets\n",
    "We import the csv file we have saved above and create the iterators for Train set, Validation set and Test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns to read.\n",
    "#text_field will contain our text from the movie reviews\n",
    "text_field = Field(lower=True, include_lengths=True, batch_first=True)\n",
    "#label_field will contain the labels respective to each review\n",
    "label_field = Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.float)\n",
    "#final list containing the labels and the text\n",
    "fields = [('sentiment', label_field), ('review', text_field)]\n",
    "\n",
    "#we use TabularDataset to create a Dataset for training, validation and testing purpose, the path of each file is passed as the argument\n",
    "train, valid, test = TabularDataset.splits(path=destination_folder, train='train.csv', validation='valid.csv', test='test.csv',\n",
    "                                           format='CSV', fields=fields, skip_header=True)\n",
    "\n",
    "#BucketIterator is used to create iterations of the complete sets in the defined batch sizes\n",
    "#Batch size is the number of examples after which the weights of the network are updated\n",
    "#A smaller batch size means the weights of the network are updated more freqeuntly and a large batch size would mean less updates\n",
    "train_iter = BucketIterator(train, batch_size=64, sort_key=lambda x: len(x.review),\n",
    "                            device=device, sort=True, sort_within_batch=True)\n",
    "valid_iter = BucketIterator(valid, batch_size=64, sort_key=lambda x: len(x.review),\n",
    "                            device=device, sort=True, sort_within_batch=True)\n",
    "test_iter = BucketIterator(test, batch_size=64, sort_key=lambda x: len(x.review),\n",
    "                            device=device, sort=True, sort_within_batch=True)\n",
    "\n",
    "# Vocabulary\n",
    "#this creates the vocabulary for our model, any word that occurs more than three times is added to our vocaublary file\n",
    "text_field.build_vocab(train, min_freq=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.1.2 Creating the model architecture\n",
    "We have defined a simple LSTM model having 128 units, with a bidirecational layer. This is followed be a Dropout layer which drops off random words from training sentences. In the end we have a linear layer, where input units are set as 2 * output units of LSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, dimension=128):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(len(text_field.vocab), 500)\n",
    "        self.dimension = dimension\n",
    "        self.lstm = nn.LSTM(input_size=500,\n",
    "                            hidden_size=dimension,\n",
    "                            num_layers=1,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "\n",
    "        self.fc = nn.Linear(2*dimension, 1)\n",
    "\n",
    "    def forward(self, text, text_len):\n",
    "\n",
    "        text_emb = self.embedding(text)\n",
    "\n",
    "        packed_input = pack_padded_sequence(text_emb, text_len, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, _ = self.lstm(packed_input)\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "        out_forward = output[range(len(output)), text_len - 1, :self.dimension]\n",
    "        out_reverse = output[:, 0, self.dimension:]\n",
    "        out_reduced = torch.cat((out_forward, out_reverse), 1)\n",
    "        text_fea = self.drop(out_reduced)\n",
    "\n",
    "        text_fea = self.fc(text_fea)\n",
    "        text_fea = torch.squeeze(text_fea, 1)\n",
    "        text_out = torch.sigmoid(text_fea)\n",
    "\n",
    "        return text_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.1.3 Saving the model\n",
    "We use the save state dictionary method to save a model, in this case, a dictionary is created where the keys are the layer names and the value pairs contain the weight matrix for the layer.  \n",
    "While using this method, each time we want to load a model, we need to define a copy of the original model which will hold the loaded weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(save_path, model, optimizer, valid_loss):\n",
    "\n",
    "    if save_path == None:\n",
    "        return\n",
    "    \n",
    "    state_dict = {'model_state_dict': model.state_dict(),\n",
    "                  'optimizer_state_dict': optimizer.state_dict(),\n",
    "                  'valid_loss': valid_loss}\n",
    "    \n",
    "    torch.save(state_dict, save_path)\n",
    "    print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "\n",
    "def load_checkpoint(load_path, model, optimizer):\n",
    "\n",
    "    if load_path==None:\n",
    "        return\n",
    "    \n",
    "    state_dict = torch.load(load_path, map_location=device)\n",
    "    print(f'Model loaded from <== {load_path}')\n",
    "    \n",
    "    model.load_state_dict(state_dict['model_state_dict'])\n",
    "    optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n",
    "    \n",
    "    return state_dict['valid_loss']\n",
    "\n",
    "\n",
    "def save_metrics(save_path, train_loss_list, valid_loss_list, global_steps_list):\n",
    "\n",
    "    if save_path == None:\n",
    "        return\n",
    "    \n",
    "    state_dict = {'train_loss_list': train_loss_list,\n",
    "                  'valid_loss_list': valid_loss_list,\n",
    "                  'global_steps_list': global_steps_list}\n",
    "    \n",
    "    torch.save(state_dict, save_path)\n",
    "    print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "\n",
    "def load_metrics(load_path):\n",
    "\n",
    "    if load_path==None:\n",
    "        return\n",
    "    \n",
    "    state_dict = torch.load(load_path, map_location=device)\n",
    "    print(f'Model loaded from <== {load_path}')\n",
    "    \n",
    "    return state_dict['train_loss_list'], state_dict['valid_loss_list'], state_dict['global_steps_list']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.1.4 Training the Model\n",
    "In this section we add the training part of our model, that trains it on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,\n",
    "          optimizer,\n",
    "          criterion = nn.BCELoss(),\n",
    "          train_loader = train_iter,\n",
    "          valid_loader = valid_iter,\n",
    "          num_epochs = 5,\n",
    "          eval_every = len(train_iter) // 2,\n",
    "          file_path = destination_folder,\n",
    "          best_valid_loss = float(\"Inf\")):\n",
    "    \n",
    "    # initialize running values\n",
    "    running_loss = 0.0\n",
    "    valid_running_loss = 0.0\n",
    "    global_step = 0\n",
    "    train_loss_list = []\n",
    "    valid_loss_list = []\n",
    "    global_steps_list = []\n",
    "\n",
    "    # training loop\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for (labels, (title, title_len)), _ in train_loader:           \n",
    "            labels = labels.to(device)\n",
    "            title = title.to(device)\n",
    "            title_len = title_len.to(device)\n",
    "            output = model(title, title_len)\n",
    "\n",
    "            loss = criterion(output, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # update running values\n",
    "            running_loss += loss.item()\n",
    "            global_step += 1\n",
    "\n",
    "            # evaluation step\n",
    "            if global_step % eval_every == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():                    \n",
    "                  # validation loop\n",
    "                  for (labels, (title, title_len)), _ in valid_loader:\n",
    "                      labels = labels.to(device)\n",
    "                      title = title.to(device)\n",
    "                      title_len = title_len.to(device)\n",
    "                      output = model(title, title_len)\n",
    "\n",
    "                      loss = criterion(output, labels)\n",
    "                      valid_running_loss += loss.item()\n",
    "\n",
    "                # evaluation\n",
    "                average_train_loss = running_loss / eval_every\n",
    "                average_valid_loss = valid_running_loss / len(valid_loader)\n",
    "                train_loss_list.append(average_train_loss)\n",
    "                valid_loss_list.append(average_valid_loss)\n",
    "                global_steps_list.append(global_step)\n",
    "\n",
    "                # resetting running values\n",
    "                running_loss = 0.0                \n",
    "                valid_running_loss = 0.0\n",
    "                model.train()\n",
    "\n",
    "                # print progress\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n",
    "                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_loader),\n",
    "                              average_train_loss, average_valid_loss))\n",
    "                \n",
    "                # checkpoint\n",
    "                if best_valid_loss > average_valid_loss:\n",
    "                    best_valid_loss = average_valid_loss\n",
    "                    save_checkpoint(file_path + '\\model.pt', model, optimizer, best_valid_loss)\n",
    "                    save_metrics(file_path + '\\metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
    "    \n",
    "    save_metrics(file_path + '\\metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
    "    print('Finished Training!')\n",
    "\n",
    "\n",
    "model = LSTM().to(device)\n",
    "pretrainedPath = torch.load('modelFinal.pth')\n",
    "model.load_state_dict(pretrainedPath, strict=False)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "\n",
    "train(model=model, optimizer=optimizer, num_epochs=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.1.4 Testing the Model\n",
    "In this section we add the testing part of our model, that tests it on the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, version='title', threshold=0.5):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (labels, (title, title_len)), _ in test_loader:           \n",
    "            labels = labels.to(device)\n",
    "            title = title.to(device)\n",
    "            title_len = title_len.to(device)\n",
    "            output = model(title, title_len)\n",
    "\n",
    "            output = (output > threshold).int()\n",
    "            y_pred.extend(output.tolist())\n",
    "            y_true.extend(labels.tolist())\n",
    "    \n",
    "    print('Classification Report:')\n",
    "    print(classification_report(y_true, y_pred, labels=[1,0], digits=4))\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[1,0])\n",
    "    ax= plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, ax = ax, cmap='Blues', fmt=\"d\")\n",
    "\n",
    "    ax.set_title('Confusion Matrix')\n",
    "\n",
    "    ax.set_xlabel('Predicted Labels')\n",
    "    ax.set_ylabel('True Labels')\n",
    "\n",
    "    ax.xaxis.set_ticklabels(['FAKE', 'REAL'])\n",
    "    ax.yaxis.set_ticklabels(['FAKE', 'REAL'])\n",
    "    \n",
    "    \n",
    "best_model = LSTM().to(device)\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=0.001)\n",
    "\n",
    "load_checkpoint(destination_folder + '\\model.pt', best_model, optimizer)\n",
    "evaluate(best_model, test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final save for the model\n",
    "torch.save(model.state_dict(), \"modelFinalTransfer.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [PyTroch LSTM Tutorial: Official Documentation](https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html)\n",
    "* [LSTMs and their uses](https://d2l.ai/chapter_recurrent-modern/lstm.html)\n",
    "* [A basic introduction to LSTM: Blog](https://blog.floydhub.com/long-short-term-memory-from-zero-to-hero-with-pytorch/)\n",
    "* [An introduction to Text Pre-Processing](https://d2l.ai/chapter_recurrent-neural-networks/text-preprocessing.html)\n",
    "* [Training PyTorch Classifier: Official Documentation](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
